{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a1ee40",
   "metadata": {},
   "source": [
    "# Creation of CaloChallenge 2022 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54aecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1f507",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e466dcf8",
   "metadata": {},
   "source": [
    "### Photons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210faa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with energy 256.0.\n",
      "Done with energy 512.0.\n",
      "Done with energy 1024.0.\n",
      "Done with energy 2048.0.\n",
      "Done with energy 4096.0.\n",
      "Done with energy 8192.0.\n",
      "Done with energy 16384.0.\n",
      "Done with energy 32768.0.\n",
      "Done with energy 65536.0.\n",
      "Done with energy 131072.0.\n",
      "Done with energy 262144.0.\n",
      "Done with energy 524288.0.\n",
      "Done with energy 1048576.0.\n",
      "Done with energy 2097152.0.\n",
      "Done with energy 4194304.0.\n",
      "Done with writing file 1\n",
      "Done with writing file 2\n",
      "Done with writing file 3\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Based on high-stats data of https://opendata.cern.ch/record/15012# \n",
    "    steps:\n",
    "        - load csv\n",
    "        - check for nans\n",
    "        - save incident energies and showers to an array\n",
    "        - create m hdf5 files, each with N events (size of actual GAN training sets)\n",
    "        - have the samples shuffled before saving\n",
    "\n",
    "\"\"\"\n",
    "folder = '../../../../ML_source/CaloChallenge/photons_samples_highStat/'\n",
    "\n",
    "# number of events in the training dataset of ATLAS\n",
    "# even though we have more events in the high-stats dataset, we only use this reduced amount\n",
    "num_events = {256: 10000, 512: 10000, 1024: 10000, 2048: 10000, 4096: 10000, 8192: 10000,\n",
    "              16384: 10000, 32768: 10000, 65536: 10000, 131072: 10000, 262144: 10000, \n",
    "              524288: 5000, 1048576: 3000, 2097152: 2000, 4194304: 1000}\n",
    "\n",
    "# number of output files generated with event numbers as specified above.\n",
    "# despite of what is written above, we are interested in having more data for evaluating the generative models.\n",
    "num_datasets = 3\n",
    "\n",
    "energies = [None for _ in range(num_datasets)]\n",
    "showers = [None for _ in range(num_datasets)]\n",
    "\n",
    "for i, n in enumerate(range(8,23)):\n",
    "    energy = float(2**n)\n",
    "    file_name = folder+'pid22_E'+str(2**n)+'_eta_20_25_voxalisation.csv'\n",
    "    loaded_array = pd.read_csv(file_name, header=None).to_numpy()\n",
    "    if np.isnan(loaded_array[:num_events[energy]]).any():\n",
    "        raise ValueError(\"Dataset contains NaNs!\")\n",
    "    assert num_datasets*num_events[energy] <= len(loaded_array), \"Not enough events in source file for E = {} MeV!\".format(energy)\n",
    "    for dataset_nr in range(num_datasets):\n",
    "        if i == 0:\n",
    "            energies[dataset_nr] = energy*np.ones(num_events[energy])\n",
    "            showers[dataset_nr] = loaded_array[num_events[energy]*(dataset_nr):num_events[energy]*(dataset_nr+1)]\n",
    "        else:\n",
    "            energies[dataset_nr] = np.append(energies[dataset_nr], energy*np.ones(num_events[energy]))\n",
    "            showers[dataset_nr] = np.append(showers[dataset_nr], loaded_array[num_events[energy]*(dataset_nr):num_events[energy]*(dataset_nr+1)], axis=0)\n",
    "    print(\"Done with energy {}.\".format(energy))\n",
    "\n",
    "for i in range(num_datasets):\n",
    "    dataset_file = h5py.File(folder+'dataset_1_photons_{}.hdf5'.format(int(i+1)), 'w')\n",
    "\n",
    "    shuffled_idx = np.arange(len(energies[0]))\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "\n",
    "    dataset_file.create_dataset('incident_energies', \n",
    "                                data=energies[i].clip(min=0.).reshape(len(energies[i]), -1)[shuffled_idx], \n",
    "                                compression='gzip')\n",
    "    dataset_file.create_dataset('showers', \n",
    "                                data=showers[i].clip(min=0.).reshape(len(showers[i]), -1)[shuffled_idx], \n",
    "                                compression='gzip')\n",
    "    print(\"Done with writing file {}\".format(i+1))\n",
    "    dataset_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8094c0f6",
   "metadata": {},
   "source": [
    "### Pions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "091cc258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 533) (10000,)\n",
      "(109998, 533) (109998,)\n",
      "Done with energy 256.0.\n",
      "(10000, 533) (10000,)\n",
      "(109999, 533) (109999,)\n",
      "Done with energy 512.0.\n",
      "(10000, 533) (10000,)\n",
      "(110000, 533) (110000,)\n",
      "Done with energy 1024.0.\n",
      "(10000, 533) (10000,)\n",
      "(110000, 533) (110000,)\n",
      "Done with energy 2048.0.\n",
      "(10000, 533) (10000,)\n",
      "(110000, 533) (110000,)\n",
      "Done with energy 4096.0.\n",
      "(10000, 533) (10000,)\n",
      "(110000, 533) (110000,)\n",
      "Done with energy 8192.0.\n",
      "(10000, 533) (10000,)\n",
      "(109900, 533) (109900,)\n",
      "Done with energy 16384.0.\n",
      "(10000, 533) (10000,)\n",
      "(110000, 533) (110000,)\n",
      "Done with energy 32768.0.\n",
      "(10000, 533) (10000,)\n",
      "(59900, 533) (59900,)\n",
      "Done with energy 65536.0.\n",
      "(10000, 533) (10000,)\n",
      "(39700, 533) (39700,)\n",
      "Done with energy 131072.0.\n",
      "(10000, 533) (10000,)\n",
      "(19800, 533) (19800,)\n",
      "Done with energy 262144.0.\n",
      "(5000, 533) (5000,)\n",
      "(14850, 533) (14850,)\n",
      "Done with energy 524288.0.\n",
      "(3000, 533) (3000,)\n",
      "(12700, 533) (12700,)\n",
      "Done with energy 1048576.0.\n",
      "(2000, 533) (2000,)\n",
      "(11477, 533) (11477,)\n",
      "Done with energy 2097152.0.\n",
      "(230, 533) (230,)\n",
      "(5215, 533) (5215,)\n",
      "Done with energy 4194304.0.\n",
      "(120800, 533) (120800,)\n",
      "(120800, 533) (120800,)\n",
      "Done with writing file 1\n",
      "Done with writing file 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Based on normal and high-stats data of https://opendata.cern.ch/record/15012# \n",
    "    steps:\n",
    "        - load csv\n",
    "        - check for nans\n",
    "        - save incident energies and showers to an array\n",
    "        - load separately generated .pkl files with indices that remove doubled showers\n",
    "        - create 2 hdf5 files, each with num_events events (about the size of actual GAN training sets)\n",
    "        - have the samples shuffled before saving\n",
    "\n",
    "\"\"\"\n",
    "folder = '../../../../ML_source/CaloChallenge/pion_samples/'\n",
    "folder2 = '../../../../ML_source/CaloChallenge/pions_samples_highStat/'\n",
    "\n",
    "# number of events in the training dataset of ATLAS\n",
    "# even though we have more events in the high-stats dataset, we only use this reduced amount\n",
    "# number of events for 262144 MeV needed to be reduced, as highStats dataset did not contain enough events\n",
    "\n",
    "num_events = {256: 10000, 512: 10000, 1024: 10000, 2048: 10000, 4096: 10000, 8192: 10000,\n",
    "              16384: 10000, 32768: 10000, 65536: 10000, 131072: 10000, 262144: 9800, \n",
    "              524288: 5000, 1048576: 3000, 2097152: 2000, 4194304: 1000}\n",
    "\n",
    "# created and saved in high-stat-test-pions.ipynb\n",
    "with open('../indices_old_file.pkl', 'rb') as f:\n",
    "    good_events_old = pickle.load(f)\n",
    "    \n",
    "with open('../indices_new_file.pkl', 'rb') as f:\n",
    "    good_events_new = pickle.load(f)\n",
    "    \n",
    "energies = []\n",
    "showers = []\n",
    "\n",
    "final_showers_1 = []\n",
    "final_showers_2 = []\n",
    "final_energies_1 = []\n",
    "final_energies_2 = []\n",
    "\n",
    "for i, n in enumerate(range(8,23)):\n",
    "    energy = float(2**n)\n",
    "    old_file_name = f\"{folder}pid211_E{2**n}_eta_20_25_voxalisation.csv\"\n",
    "    old_loaded_array = pd.read_csv(old_file_name, header=None).to_numpy()[good_events_old[energy]]\n",
    "    if np.isnan(old_loaded_array).any():\n",
    "        raise ValueError(\"Old dataset contains NaNs!\")\n",
    "    new_file_name = f\"{folder2}pid211_E{2**n}_eta_20_25_voxalisation.csv\"\n",
    "    new_loaded_array = pd.read_csv(new_file_name, header=None).to_numpy()[good_events_new[energy]]\n",
    "    if np.isnan(new_loaded_array).any():\n",
    "        raise ValueError(\"New dataset contains NaNs!\")\n",
    "\n",
    "    showers.append(old_loaded_array)\n",
    "    energies.append(energy*np.ones(len(old_loaded_array)))\n",
    "    print(showers[-1].shape, energies[-1].shape)\n",
    "\n",
    "    showers[-1] = np.vstack([showers[-1], new_loaded_array])\n",
    "    energies[-1] = np.array([*energies[-1], *(energy*np.ones(len(new_loaded_array)))])\n",
    "    print(showers[-1].shape, energies[-1].shape)\n",
    "\n",
    "    #showers[-1] = showers[-1][:2*num_events[2**n]]\n",
    "    #energies[-1] = energies[-1][:2*num_events[2**n]]\n",
    "    #print(showers[-1].shape, energies[-1].shape)\n",
    "    final_showers_1.append(showers[-1][:num_events[2**n]])\n",
    "    final_showers_2.append(showers[-1][num_events[2**n]:2*num_events[2**n]])\n",
    "    final_energies_1.append(energies[-1][:num_events[2**n]])\n",
    "    final_energies_2.append(energies[-1][num_events[2**n]:2*num_events[2**n]])\n",
    "\n",
    "    print(\"Done with energy {}.\".format(energy))\n",
    "\n",
    "showers_1 = np.vstack([*final_showers_1])\n",
    "showers_2 = np.vstack([*final_showers_2])\n",
    "energies_1 = np.hstack([*final_energies_1])\n",
    "energies_2 = np.hstack([*final_energies_2])\n",
    "print(showers_1.shape, energies_1.shape)\n",
    "print(showers_2.shape, energies_2.shape)\n",
    "\n",
    "final_showers = [showers_1, showers_2]\n",
    "final_energies = [energies_1, energies_2]\n",
    "    \n",
    "for i in range(2):\n",
    "    dataset_file = h5py.File(folder2+'dataset_1_pions_highstat_{}.hdf5'.format(int(i+1)), 'w')\n",
    "\n",
    "    shuffled_idx = np.arange(len(final_energies[i]))\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "\n",
    "    dataset_file.create_dataset('incident_energies', \n",
    "                                data=final_energies[i].clip(min=0.).reshape(len(final_energies[i]), -1)[shuffled_idx], \n",
    "                                compression='gzip')\n",
    "    dataset_file.create_dataset('showers', \n",
    "                                data=final_showers[i].clip(min=0.).reshape(len(final_showers[i]), -1)[shuffled_idx], \n",
    "                                compression='gzip')\n",
    "    print(\"Done with writing file {}\".format(i+1))\n",
    "    dataset_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d5006",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b232f",
   "metadata": {},
   "source": [
    "### Electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b55da91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with reading file 1/124\n",
      "Done with reading file 2/124\n",
      "Done with reading file 3/124\n",
      "Done with reading file 4/124\n",
      "Done with reading file 5/124\n",
      "Done with reading file 6/124\n",
      "Done with reading file 7/124\n",
      "Done with reading file 8/124\n",
      "Done with reading file 9/124\n",
      "Done with reading file 10/124\n",
      "Done with reading file 11/124\n",
      "Done with reading file 12/124\n",
      "Done with reading file 13/124\n",
      "Done with reading file 14/124\n",
      "Done with reading file 15/124\n",
      "Done with reading file 16/124\n",
      "Done with reading file 17/124\n",
      "Done with reading file 18/124\n",
      "Done with reading file 19/124\n",
      "Done with reading file 20/124\n",
      "Done with reading file 21/124\n",
      "Done with reading file 22/124\n",
      "Done with reading file 23/124\n",
      "Done with reading file 24/124\n",
      "Done with reading file 25/124\n",
      "Done with reading file 26/124\n",
      "Done with reading file 27/124\n",
      "Done with reading file 28/124\n",
      "Done with reading file 29/124\n",
      "Done with reading file 30/124\n",
      "Done with reading file 31/124\n",
      "Done with reading file 32/124\n",
      "Done with reading file 33/124\n",
      "Done with reading file 34/124\n",
      "Done with reading file 35/124\n",
      "Done with reading file 36/124\n",
      "Done with reading file 37/124\n",
      "Done with reading file 38/124\n",
      "Done with reading file 39/124\n",
      "Done with reading file 40/124\n",
      "100000\n",
      "(100000, 9, 16, 45)\n",
      "(100000, 45, 9, 16)\n",
      "(100000, 45, 16, 9)\n",
      "Done with writing file 1\n",
      "Done with reading file 41/124\n",
      "Done with reading file 42/124\n",
      "Done with reading file 43/124\n",
      "Done with reading file 44/124\n",
      "Done with reading file 45/124\n",
      "Done with reading file 46/124\n",
      "Done with reading file 47/124\n",
      "Done with reading file 48/124\n",
      "Done with reading file 49/124\n",
      "Done with reading file 50/124\n",
      "Done with reading file 51/124\n",
      "Done with reading file 52/124\n",
      "Done with reading file 53/124\n",
      "Done with reading file 54/124\n",
      "Done with reading file 55/124\n",
      "Done with reading file 56/124\n",
      "Done with reading file 57/124\n",
      "Done with reading file 58/124\n",
      "Done with reading file 59/124\n",
      "Done with reading file 60/124\n",
      "Done with reading file 61/124\n",
      "Done with reading file 62/124\n",
      "Done with reading file 63/124\n",
      "Done with reading file 64/124\n",
      "Done with reading file 65/124\n",
      "Done with reading file 66/124\n",
      "Done with reading file 67/124\n",
      "Done with reading file 68/124\n",
      "Done with reading file 69/124\n",
      "Done with reading file 70/124\n",
      "Done with reading file 71/124\n",
      "Done with reading file 72/124\n",
      "Done with reading file 73/124\n",
      "Done with reading file 74/124\n",
      "Done with reading file 75/124\n",
      "Done with reading file 76/124\n",
      "Done with reading file 77/124\n",
      "Done with reading file 78/124\n",
      "Done with reading file 79/124\n",
      "Done with reading file 80/124\n",
      "100000\n",
      "(100000, 9, 16, 45)\n",
      "(100000, 45, 9, 16)\n",
      "(100000, 45, 16, 9)\n",
      "Done with writing file 2\n",
      "Done with reading file 81/124\n",
      "Done with reading file 82/124\n",
      "Done with reading file 83/124\n",
      "Done with reading file 84/124\n",
      "Done with reading file 85/124\n",
      "Done with reading file 86/124\n",
      "Done with reading file 87/124\n",
      "Done with reading file 88/124\n",
      "Done with reading file 89/124\n",
      "Done with reading file 90/124\n",
      "Done with reading file 91/124\n",
      "Done with reading file 92/124\n",
      "Done with reading file 93/124\n",
      "Done with reading file 94/124\n",
      "Done with reading file 95/124\n",
      "Done with reading file 96/124\n",
      "Done with reading file 97/124\n",
      "Done with reading file 98/124\n",
      "Done with reading file 99/124\n",
      "Done with reading file 100/124\n",
      "Done with reading file 101/124\n",
      "Done with reading file 102/124\n",
      "Done with reading file 103/124\n",
      "Done with reading file 104/124\n",
      "Done with reading file 105/124\n",
      "Done with reading file 106/124\n",
      "Done with reading file 107/124\n",
      "Done with reading file 108/124\n",
      "Done with reading file 109/124\n",
      "Done with reading file 110/124\n",
      "Done with reading file 111/124\n",
      "Done with reading file 112/124\n",
      "Done with reading file 113/124\n",
      "Done with reading file 114/124\n",
      "Done with reading file 115/124\n",
      "Done with reading file 116/124\n",
      "Done with reading file 117/124\n",
      "Done with reading file 118/124\n",
      "Done with reading file 119/124\n",
      "Done with reading file 120/124\n",
      "100000\n",
      "(100000, 9, 16, 45)\n",
      "(100000, 45, 9, 16)\n",
      "(100000, 45, 16, 9)\n",
      "Done with writing file 3\n",
      "Done with reading file 121/124\n",
      "Done with reading file 122/124\n",
      "Done with reading file 123/124\n",
      "Done with reading file 124/124\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Based on CaloChallenge_Dataset/Dataset2 of Dalila, taken from https://cernbox.cern.ch/index.php/s/KwFvdbub9QNP6qA\n",
    "    steps:\n",
    "        - load existing hdf5 files\n",
    "        - read out energy and shower\n",
    "        - concatenate to list of 150k showers\n",
    "        - transform shape (num_events, r_bins, alpha_bins, layer_id) to \n",
    "          (num_events, layer_id, alpha_bins, r_bins) as in dataset 1, then flatten last dimensions\n",
    "        - rescale by sampling_fraction, as given by Anna\n",
    "        - write to new hdf5 files\n",
    "\n",
    "\"\"\"\n",
    "folder = '../../../../ML_source/CaloChallenge/Dataset2_cont_energy/'\n",
    "sampling_fraction = 1./0.033\n",
    "\n",
    "energy = []\n",
    "shower = []\n",
    "\n",
    "output_nr = 1\n",
    "\n",
    "for idx, source_file in enumerate(glob.glob(folder+'*')):\n",
    "    data_source = h5py.File(source_file, 'r')\n",
    "\n",
    "    for key in data_source[\"Angle_90\"].keys():\n",
    "        energy.append(float(key))\n",
    "        shower.append(data_source[\"Angle_90\"][key][:])\n",
    "    data_source.close()\n",
    "    print(\"Done with reading file {}/{}\".format(idx+1, len(glob.glob(folder+'*'))-output_nr+1))\n",
    "    if idx % 40 == 39:\n",
    "        energy = np.array(energy)\n",
    "        print(len(energy))\n",
    "        shower = np.array(shower)\n",
    "        print(shower.shape)\n",
    "        shower = np.moveaxis(shower, 3, 1)\n",
    "        print(shower.shape)\n",
    "        shower = np.moveaxis(shower, 3, 2)\n",
    "        print(shower.shape)\n",
    "        \n",
    "        shuffled_idx = np.arange(len(energy))\n",
    "        np.random.shuffle(shuffled_idx)\n",
    "        \n",
    "        dataset_file = h5py.File(folder + 'dataset_2_{}.hdf5'.format(output_nr), 'w')\n",
    "        dataset_file.create_dataset('incident_energies', data=energy.clip(min=0.).reshape(len(energy), -1)[shuffled_idx], compression='gzip')\n",
    "        dataset_file.create_dataset('showers', data=sampling_fraction*shower.clip(min=0.).reshape(len(shower), -1)[shuffled_idx], compression='gzip')\n",
    "        print(\"Done with writing file {}\".format(output_nr))\n",
    "        dataset_file.close()\n",
    "        output_nr += 1\n",
    "        energy = []\n",
    "        shower = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b44abb",
   "metadata": {},
   "source": [
    "## Dataset 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eda4fd",
   "metadata": {},
   "source": [
    "### Electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa98af7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with reading file 1/127\n",
      "Done with reading file 2/127\n",
      "Done with reading file 3/127\n",
      "Done with reading file 4/127\n",
      "Done with reading file 5/127\n",
      "Done with reading file 6/127\n",
      "Done with reading file 7/127\n",
      "Done with reading file 8/127\n",
      "Done with reading file 9/127\n",
      "Done with reading file 10/127\n",
      "Done with reading file 11/127\n",
      "Done with reading file 12/127\n",
      "Done with reading file 13/127\n",
      "Done with reading file 14/127\n",
      "Done with reading file 15/127\n",
      "Done with reading file 16/127\n",
      "Done with reading file 17/127\n",
      "Done with reading file 18/127\n",
      "Done with reading file 19/127\n",
      "Done with reading file 20/127\n",
      "50000\n",
      "(50000, 18, 50, 45)\n",
      "(50000, 45, 18, 50)\n",
      "(50000, 45, 50, 18)\n",
      "Done with writing file 1\n",
      "Done with reading file 21/128\n",
      "Done with reading file 22/128\n",
      "Done with reading file 23/128\n",
      "Done with reading file 24/128\n",
      "Done with reading file 25/128\n",
      "Done with reading file 26/128\n",
      "Done with reading file 27/128\n",
      "Done with reading file 28/128\n",
      "Done with reading file 29/128\n",
      "Done with reading file 30/128\n",
      "Done with reading file 31/128\n",
      "Done with reading file 32/128\n",
      "Done with reading file 33/128\n",
      "Done with reading file 34/128\n",
      "Done with reading file 35/128\n",
      "Done with reading file 36/128\n",
      "Done with reading file 37/128\n",
      "Done with reading file 38/128\n",
      "Done with reading file 39/128\n",
      "Done with reading file 40/128\n",
      "50000\n",
      "(50000, 18, 50, 45)\n",
      "(50000, 45, 18, 50)\n",
      "(50000, 45, 50, 18)\n",
      "Done with writing file 2\n",
      "Done with reading file 41/129\n",
      "Done with reading file 42/129\n",
      "Done with reading file 43/129\n",
      "Done with reading file 44/129\n",
      "Done with reading file 45/129\n",
      "Done with reading file 46/129\n",
      "Done with reading file 47/129\n",
      "Done with reading file 48/129\n",
      "Done with reading file 49/129\n",
      "Done with reading file 50/129\n",
      "Done with reading file 51/129\n",
      "Done with reading file 52/129\n",
      "Done with reading file 53/129\n",
      "Done with reading file 54/129\n",
      "Done with reading file 55/129\n",
      "Done with reading file 56/129\n",
      "Done with reading file 57/129\n",
      "Done with reading file 58/129\n",
      "Done with reading file 59/129\n",
      "Done with reading file 60/129\n",
      "50000\n",
      "(50000, 18, 50, 45)\n",
      "(50000, 45, 18, 50)\n",
      "(50000, 45, 50, 18)\n",
      "Done with writing file 3\n",
      "Done with reading file 61/130\n",
      "Done with reading file 62/130\n",
      "Done with reading file 63/130\n",
      "Done with reading file 64/130\n",
      "Done with reading file 65/130\n",
      "Done with reading file 66/130\n",
      "Done with reading file 67/130\n",
      "Done with reading file 68/130\n",
      "Done with reading file 69/130\n",
      "Done with reading file 70/130\n",
      "Done with reading file 71/130\n",
      "Done with reading file 72/130\n",
      "Done with reading file 73/130\n",
      "Done with reading file 74/130\n",
      "Done with reading file 75/130\n",
      "Done with reading file 76/130\n",
      "Done with reading file 77/130\n",
      "Done with reading file 78/130\n",
      "Done with reading file 79/130\n",
      "Done with reading file 80/130\n",
      "50000\n",
      "(50000, 18, 50, 45)\n",
      "(50000, 45, 18, 50)\n",
      "(50000, 45, 50, 18)\n",
      "Done with writing file 4\n",
      "Done with reading file 81/131\n",
      "Done with reading file 82/131\n",
      "Done with reading file 83/131\n",
      "Done with reading file 84/131\n",
      "Done with reading file 85/131\n",
      "Done with reading file 86/131\n",
      "Done with reading file 87/131\n",
      "Done with reading file 88/131\n",
      "Done with reading file 89/131\n",
      "Done with reading file 90/131\n",
      "Done with reading file 91/131\n",
      "Done with reading file 92/131\n",
      "Done with reading file 93/131\n",
      "Done with reading file 94/131\n",
      "Done with reading file 95/131\n",
      "Done with reading file 96/131\n",
      "Done with reading file 97/131\n",
      "Done with reading file 98/131\n",
      "Done with reading file 99/131\n",
      "Done with reading file 100/131\n",
      "50000\n",
      "(50000, 18, 50, 45)\n",
      "(50000, 45, 18, 50)\n",
      "(50000, 45, 50, 18)\n",
      "Done with writing file 5\n",
      "Done with reading file 101/132\n",
      "Done with reading file 102/132\n",
      "Done with reading file 103/132\n",
      "Done with reading file 104/132\n",
      "Done with reading file 105/132\n",
      "Done with reading file 106/132\n",
      "Done with reading file 107/132\n",
      "Done with reading file 108/132\n",
      "Done with reading file 109/132\n",
      "Done with reading file 110/132\n",
      "Done with reading file 111/132\n",
      "Done with reading file 112/132\n",
      "Done with reading file 113/132\n",
      "Done with reading file 114/132\n",
      "Done with reading file 115/132\n",
      "Done with reading file 116/132\n",
      "Done with reading file 117/132\n",
      "Done with reading file 118/132\n",
      "Done with reading file 119/132\n",
      "Done with reading file 120/132\n",
      "50000\n",
      "(50000, 18, 50, 45)\n",
      "(50000, 45, 18, 50)\n",
      "(50000, 45, 50, 18)\n",
      "Done with writing file 6\n",
      "Done with reading file 121/133\n",
      "Done with reading file 122/133\n",
      "Done with reading file 123/133\n",
      "Done with reading file 124/133\n",
      "Done with reading file 125/133\n",
      "Done with reading file 126/133\n",
      "Done with reading file 127/133\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Based on CaloChallenge_Dataset/Dataset3 of Dalila, taken from https://cernbox.cern.ch/index.php/s/KwFvdbub9QNP6qA\n",
    "    steps:\n",
    "        - load existing hdf5 files\n",
    "        - read out energy and shower\n",
    "        - concatenate to list of 50k showers\n",
    "        - transform shape (num_events, r_bins, alpha_bins, layer_id) to \n",
    "          (num_events, layer_id, alpha_bins, r_bins) as in dataset 1, then flatten last dimensions\n",
    "        - rescale by sampling_fraction, as given by Anna\n",
    "        - write to new hdf5 files\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "folder = '../../../../ML_source/CaloChallenge/Dataset3_cont_energy/'\n",
    "sampling_fraction = 1./0.033\n",
    "\n",
    "energy = []\n",
    "shower = []\n",
    "\n",
    "output_nr = 1\n",
    "\n",
    "for idx, source_file in enumerate(glob.glob(folder+'*')):\n",
    "    data_source = h5py.File(source_file, 'r')\n",
    "\n",
    "    for key in data_source[\"Angle_90\"].keys():\n",
    "        energy.append(float(key))\n",
    "        shower.append(data_source[\"Angle_90\"][key][:])\n",
    "    data_source.close()\n",
    "    print(\"Done with reading file {}/{}\".format(idx+1, len(glob.glob(folder+'*'))-output_nr+1))\n",
    "    if idx % 20 == 19:\n",
    "        energy = np.array(energy)\n",
    "        print(len(energy))\n",
    "        shower = np.array(shower)\n",
    "        print(shower.shape)\n",
    "        shower = np.moveaxis(shower, 3, 1)\n",
    "        print(shower.shape)\n",
    "        shower = np.moveaxis(shower, 3, 2)\n",
    "        print(shower.shape)\n",
    "        \n",
    "        shuffled_idx = np.arange(len(energy))\n",
    "        np.random.shuffle(shuffled_idx)\n",
    "\n",
    "\n",
    "        dataset_file = h5py.File(folder + 'dataset_3_{}.hdf5'.format(output_nr), 'w')\n",
    "        dataset_file.create_dataset('incident_energies', data=energy.clip(min=0.).reshape(len(energy), -1)[shuffled_idx], compression='gzip')\n",
    "        dataset_file.create_dataset('showers', data=sampling_fraction*shower.clip(min=0.).reshape(len(shower), -1)[shuffled_idx], compression='gzip')\n",
    "        print(\"Done with writing file {}\".format(output_nr))\n",
    "        dataset_file.close()\n",
    "        output_nr += 1\n",
    "        energy = []\n",
    "        shower = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7c6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
